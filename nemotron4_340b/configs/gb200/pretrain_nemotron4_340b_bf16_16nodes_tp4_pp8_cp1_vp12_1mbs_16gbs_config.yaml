_partial_: true
_target_: nemo.collections.llm.api.pretrain
data:
  _target_: nemo.collections.llm.gpt.data.mock.MockDataModule
  global_batch_size: 16
  micro_batch_size: 1
  num_train_samples: 320000
  seq_length: 4096
  tokenizer:
    _target_: nemo.collections.nlp.modules.common.tokenizer_utils.get_nmt_tokenizer
    library: 'null'
    model_name: NullTokenizer
    vocab_size: 256000
log:
  _target_: nemo.lightning.nemo_logger.NeMoLogger
  ckpt: null
  log_dir: null
  name: default
  tensorboard: null
  wandb: null
model:
  _target_: nemo.collections.llm.gpt.model.nemotron.NemotronModel
  config:
    _target_: nemo.collections.llm.gpt.model.nemotron.Nemotron4Config340B
    enable_cuda_graph: false
  tokenizer:
    _target_: nemo.collections.nlp.modules.common.tokenizer_utils.get_nmt_tokenizer
    library: 'null'
    model_name: NullTokenizer
    vocab_size: 256000
optim:
  _target_: nemo.lightning.pytorch.optim.megatron.MegatronOptimizerModule
  config:
    _target_: megatron.core.optimizer.optimizer_config.OptimizerConfig
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.0e-05
    bf16: true
    clip_grad: 1.0
    fp16: false
    lr: 0.0001
    optimizer: adam
    use_distributed_optimizer: true
    weight_decay: 0.1
  lr_scheduler:
    _target_: nemo.lightning.pytorch.optim.lr_scheduler.CosineAnnealingScheduler
    constant_steps: 0
    min_lr: 1.0e-05
    warmup_steps: 500
resume:
  _target_: nemo.lightning.resume.AutoResume
  resume_if_exists: true
  resume_ignore_no_checkpoint: true
trainer:
  _target_: nemo.lightning.pytorch.trainer.Trainer
  accelerator: gpu
  accumulate_grad_batches: 1
  callbacks:
  - _target_: nemo.utils.exp_manager.TimingCallback
  - _target_: nemo.lightning.pytorch.callbacks.garbage_collection.GarbageCollectionCallback
    gc_interval_train: 100
    gc_interval_val: 100
  - _target_: nemo.lightning.pytorch.callbacks.megatron_comm_overlap.MegatronCommOverlapCallback
    defer_embedding_wgrad_compute: true
    overlap_param_gather_with_optimizer_step: true
    tp_comm_overlap: true
    wgrad_deferral_limit: 22
  - _target_: nemo.lightning.pytorch.callbacks.flops_callback.FLOPsMeasurementCallback
    data_config:
      _target_: nemo.collections.llm.gpt.data.mock.MockDataModule
      global_batch_size: 16
      micro_batch_size: 1
      num_train_samples: 320000
      seq_length: 4096
      tokenizer:
        _target_: nemo.collections.nlp.modules.common.tokenizer_utils.get_nmt_tokenizer
        library: 'null'
        model_name: NullTokenizer
        vocab_size: 256000
    model_config:
      _target_: nemo.collections.llm.gpt.model.nemotron.Nemotron4Config340B
      enable_cuda_graph: false
    model_name: nemotron
  devices: 4
  enable_checkpointing: false
  limit_test_batches: 32
  limit_val_batches: 0
  log_every_n_steps: 1
  logger: false
  max_steps: 20000
  num_nodes: 16
  plugins:
    _target_: nemo.lightning.pytorch.plugins.mixed_precision.MegatronMixedPrecision
    autocast_enabled: false
    grad_reduce_in_fp32: false
    params_dtype:
      _call_: false
      _target_: torch.bfloat16
    pipeline_dtype:
      _call_: false
      _target_: torch.bfloat16
    precision: bf16-mixed
  strategy:
    _target_: nemo.lightning.pytorch.strategies.megatron_strategy.MegatronStrategy
    ckpt_async_save: true
    ckpt_include_optimizer: true
    ckpt_parallel_load: true
    context_parallel_size: 1
    ddp:
      _target_: megatron.core.distributed.distributed_data_parallel_config.DistributedDataParallelConfig
      average_in_collective: true
      check_for_nan_in_grad: true
      grad_reduce_in_fp32: true
      overlap_grad_reduce: true
      overlap_param_gather: true
    expert_model_parallel_size: 1
    expert_tensor_parallel_size: null
    gradient_as_bucket_view: true
    pipeline_dtype:
      _call_: false
      _target_: torch.bfloat16
    pipeline_model_parallel_size: 8
    sequence_parallel: true
    tensor_model_parallel_size: 4
    use_te_rng_tracker: false
    virtual_pipeline_model_parallel_size: 12
  use_distributed_sampler: false
  val_check_interval: 20000
